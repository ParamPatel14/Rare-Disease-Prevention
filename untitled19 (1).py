# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCxBMhWpGcx9KUIENTRZEA2SUWtULATT
"""

import pandas as pd
import xml.etree.ElementTree as ET
import gzip
import os
import numpy as np

# 1. Simulate a dataset (for immediate development)
def create_simulated_dataset(n_patients=100):
    np.random.seed(42)
    data = {
        'patient_id': range(n_patients),
        'symptoms': [
            'seizures,weak_legs' if i % 2 == 0 else 'rashes,fatigue' for i in range(n_patients)
        ],
        'dna_mutations': [
            'CLN3' if i % 2 == 0 else 'SNRPN' for i in range(n_patients)
        ],
        'disease': [
            'Batten Disease' if i % 2 == 0 else 'Prader-Willi Syndrome' for i in range(n_patients)
        ]
    }
    df = pd.DataFrame(data)
    df.to_csv('simulated_rare_disease_data.csv', index=False)
    return df

# 2. Load Orphanet data (example for XML file)
def load_orphanet_data(file_path):
    if not os.path.exists(file_path):
        print(f"Orphanet file {file_path} not found. Please download from www.orphadata.org.")
        return None
    try:
        tree = ET.parse(file_path)
        root = tree.getroot()
        diseases = []
        for disorder in root.findall('.//Disorder'):
            orpha_code = disorder.find('OrphaCode').text
            name = disorder.find('Name').text
            phenotypes = [hpo.find('Name').text for hpo in disorder.findall('.//HPOTerm')]
            diseases.append({'orpha_code': orpha_code, 'name': name, 'phenotypes': phenotypes})
        return pd.DataFrame(diseases)
    except Exception as e:
        print(f"Error parsing Orphanet file: {e}")
        return None

# 3. Load ClinVar data (example for VCF file)
def load_clinvar_data(file_path):
    if not os.path.exists(file_path):
        print(f"ClinVar file {file_path} not found. Please download from ftp.ncbi.nlm.nih.gov/pub/clinvar/.")
        return None
    try:
        variants = []
        with gzip.open(file_path, 'rt') as f:
            for line in f:
                if line.startswith('#'):
                    continue
                fields = line.strip().split('\t')
                if len(fields) > 7:
                    gene = fields[7].split('GENEINFO=')[1].split(':')[0] if 'GENEINFO=' in fields[7] else 'Unknown'
                    disease = fields[7].split('CLNDISDB=')[1].split(',')[0] if 'CLNDISDB=' in fields[7] else 'Unknown'
                    variants.append({'gene': gene, 'disease': disease})
        return pd.DataFrame(variants)
    except Exception as e:
        print(f"Error parsing ClinVar file: {e}")
        return None

# Main execution
if __name__ == "__main__":
    # Create simulated dataset
    print("Creating simulated dataset...")
    sim_data = create_simulated_dataset()
    print("Simulated dataset preview:")
    print(sim_data.head())

    # Example: Load Orphanet data (replace with actual file path)
    orphanet_file = 'en_product1.xml'  # Update with downloaded file
    orphanet_data = load_orphanet_data(orphanet_file)
    if orphanet_data is not None:
        print("\nOrphanet data preview:")
        print(orphanet_data.head())

    # Example: Load ClinVar data (replace with actual file path)
    clinvar_file = 'clinvar_20250526.vcf.gz'  # Update with downloaded file
    clinvar_data = load_clinvar_data(clinvar_file)
    if clinvar_data is not None:
        print("\nClinVar data preview:")
        print(clinvar_data.head())

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, TFBertModel
import tensorflow as tf

# Load simulated dataset (or replace with real Orphanet/ClinVar data)
def load_data(file_path='simulated_rare_disease_data.csv'):
    return pd.read_csv(file_path)

# Encode symptoms using BERT
def get_symptom_embeddings(symptoms, tokenizer, model):
    inputs = tokenizer(symptoms, return_tensors='tf', padding=True, truncation=True, max_length=50)
    outputs = model(inputs)
    return outputs.pooler_output.numpy()  # Use pooler output as embedding

# Preprocess data
def preprocess_data(df):
    # Initialize BERT for symptom encoding
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')

    # Encode symptoms
    symptom_embeddings = np.vstack([get_symptom_embeddings(s, tokenizer, bert_model) for s in df['symptoms']])

    # Encode DNA mutations
    le_dna = LabelEncoder()
    dna_encoded = le_dna.fit_transform(df['dna_mutations']).reshape(-1, 1)

    # Encode disease labels
    le_disease = LabelEncoder()
    disease_encoded = le_disease.fit_transform(df['disease'])

    # Combine features
    X = np.hstack([symptom_embeddings, dna_encoded])
    y = disease_encoded

    return X, y, le_disease

# Main execution
if __name__ == "__main__":
    # Load data
    df = load_data()
    print("Dataset preview:")
    print(df.head())

    # Preprocess
    X, y, le_disease = preprocess_data(df)
    print("\nFeature matrix shape:", X.shape)
    print("Label array shape:", y.shape)
    print("Sample features (first patient):", X[0])
    print("Sample label (first patient):", le_disease.inverse_transform([y[0]])[0])

import tensorflow as tf
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Load preprocessed data
def load_preprocessed_data():
    df = pd.read_csv('simulated_rare_disease_data.csv')
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')
    symptom_embeddings = np.vstack([get_symptom_embeddings(s, tokenizer, bert_model) for s in df['symptoms']])
    le_dna = LabelEncoder()
    dna_encoded = le_dna.fit_transform(df['dna_mutations']).reshape(-1, 1)
    X = np.hstack([symptom_embeddings, dna_encoded])
    return X, le_dna

def get_symptom_embeddings(symptoms, tokenizer, model):
    inputs = tokenizer(symptoms, return_tensors='tf', padding=True, truncation=True, max_length=50)
    outputs = model(inputs)
    return outputs.pooler_output.numpy()

# Define VAE
class VAE(tf.keras.Model):
    def __init__(self, input_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = tf.keras.Sequential([
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(latent_dim * 2)  # Mean and log variance
        ])
        self.decoder = tf.keras.Sequential([
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(input_dim, activation='sigmoid')
        ])

    def call(self, inputs):
        mean_logvar = self.encoder(inputs)
        mean, logvar = tf.split(mean_logvar, num_or_size_splits=2, axis=1)
        epsilon = tf.random.normal(tf.shape(mean))
        z = mean + tf.exp(0.5 * logvar) * epsilon
        return self.decoder(z)

# Generate synthetic data
def generate_synthetic_data(vae, n_samples, le_dna):
    z_sample = tf.random.normal([n_samples, 10])
    X_synthetic = vae.decoder(z_sample).numpy()
    synthetic_dna = le_dna.inverse_transform(np.argmax(X_synthetic[:, -1:], axis=1))
    synthetic_data = pd.DataFrame({
        'patient_id': range(100, 100 + n_samples),
        'symptoms': ['synthetic_symptoms'] * n_samples,  # Placeholder
        'dna_mutations': synthetic_dna,
        'disease': ['Unknown'] * n_samples
    })
    return synthetic_data

# Main execution
if __name__ == "__main__":
    # Load preprocessed data
    X, le_dna = load_preprocessed_data()
    print("Feature matrix shape:", X.shape)

    # Train VAE
    vae = VAE(input_dim=X.shape[1], latent_dim=10)
    vae.compile(optimizer='adam', loss='mse')
    vae.fit(X, X, epochs=10, batch_size=32, verbose=1)

    # Generate synthetic data
    synthetic_data = generate_synthetic_data(vae, 50, le_dna)
    print("\nSynthetic dataset preview:")
    print(synthetic_data.head())
    synthetic_data.to_csv('synthetic_rare_disease_data.csv', index=False)

!pip install torch_geometric

import pandas as pd
import numpy as np
import torch
import torch_geometric as pyg
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.preprocessing import LabelEncoder

# Load data
def load_data():
    real_data = pd.read_csv('simulated_rare_disease_data.csv')
    synthetic_data = pd.read_csv('synthetic_rare_disease_data.csv')
    return real_data, synthetic_data

# Preprocess (simplified, assumes preprocessed features from Step 2)
def preprocess_data(real_data, synthetic_data):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')

    def get_symptom_embeddings(symptoms):
        inputs = tokenizer(symptoms, return_tensors='tf', padding=True, truncation=True, max_length=50)
        outputs = bert_model(inputs)
        return outputs.pooler_output.numpy()

    # Encode real data
    real_embeddings = np.vstack([get_symptom_embeddings(s) for s in real_data['symptoms']])
    le_dna = LabelEncoder()
    real_dna = le_dna.fit_transform(real_data['dna_mutations']).reshape(-1, 1)
    le_disease = LabelEncoder()
    real_labels = le_disease.fit_transform(real_data['disease'])
    X_real = np.hstack([real_embeddings, real_dna])

    # Encode synthetic data (ignore 'Unknown' labels)
    synth_embeddings = np.vstack([get_symptom_embeddings(s) for s in synthetic_data['symptoms']])
    synth_dna = le_dna.transform(synthetic_data['dna_mutations']).reshape(-1, 1)
    X_synth = np.hstack([synth_embeddings, synth_dna])

    # Combine
    X = np.vstack([X_real, X_synth])
    y = np.concatenate([real_labels, [-1] * len(synthetic_data)])  # -1 for unlabeled synthetic data
    return X, y, le_disease

# Create graph
def create_graph(X, symptoms):
    edge_index = []
    for i in range(len(symptoms)):
        for j in range(i + 1, len(symptoms)):
            if symptoms[i] == symptoms[j]:  # Connect patients with same symptoms
                edge_index.append([i, j])
                edge_index.append([j, i])
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    return Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index)

# Define GNN
class GNN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return torch.log_softmax(x, dim=1)

# Main execution
if __name__ == "__main__":
    # Load and preprocess
    real_data, synthetic_data = load_data()
    X, y, le_disease = preprocess_data(real_data, synthetic_data)
    graph_data = create_graph(X, real_data['symptoms'].tolist() + synthetic_data['symptoms'].tolist())

    # Train GNN
    model = GNN(input_dim=X.shape[1], hidden_dim=64, num_classes=len(le_disease.classes_))
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    model.train()
    for epoch in range(50):
        optimizer.zero_grad()
        out = model(graph_data)
        mask = y != -1  # Only use labeled data for loss
        loss = torch.nn.functional.nll_loss(out[mask], torch.tensor(y[mask], dtype=torch.long))
        loss.backward()
        optimizer.step()
        if epoch % 10 == 0:
            print(f'Epoch {epoch}, Loss: {loss.item()}')

    # Predict and explain
    model.eval()
    with torch.no_grad():
        pred = model(graph_data).argmax(dim=1)
        pred_diseases = le_disease.inverse_transform(pred[y != -1])

    for i, patient in real_data.iterrows():
        explanation = f"Predicted {pred_diseases[i]} due to symptoms: {patient['symptoms']} and mutation: {patient['dna_mutations']}"
        print(explanation)

import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, TFBertModel
import tensorflow as tf

# Preprocess data (adapted to handle only real data)
def preprocess_data(real_data):
    # Initialize BERT for symptom encoding
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')

    def get_symptom_embeddings(symptoms):
        inputs = tokenizer(symptoms, return_tensors='tf', padding=True, truncation=True, max_length=50)
        outputs = bert_model(inputs)
        return outputs.pooler_output.numpy()

    # Encode symptoms
    symptom_embeddings = np.vstack([get_symptom_embeddings(s) for s in real_data['symptoms']])

    # Encode DNA mutations
    le_dna = LabelEncoder()
    dna_encoded = le_dna.fit_transform(real_data['dna_mutations']).reshape(-1, 1)

    # Encode disease labels
    le_disease = LabelEncoder()
    disease_encoded = le_disease.fit_transform(real_data['disease'])

    # Combine features
    X = np.hstack([symptom_embeddings, dna_encoded])
    y = disease_encoded

    return X, y, le_disease

# Create graph
def create_graph(X, symptoms):
    edge_index = []
    for i in range(len(symptoms)):
        for j in range(i + 1, len(symptoms)):
            if symptoms[i] == symptoms[j]:  # Connect patients with same symptoms
                edge_index.append([i, j])
                edge_index.append([j, i])
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    return Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index)

# Define GNN
class GNN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return torch.log_softmax(x, dim=1)

# Load data and model
def load_data_and_model():
    real_data = pd.read_csv('simulated_rare_disease_data.csv')
    X, y, le_disease = preprocess_data(real_data)
    graph_data = create_graph(X, real_data['symptoms'].tolist())
    model = GNN(input_dim=X.shape[1], hidden_dim=64, num_classes=len(le_disease.classes_))
    return real_data, graph_data, model, le_disease

# Basic explainability
def explain_prediction(patient, prediction, le_disease):
    disease = le_disease.inverse_transform([prediction])[0]
    return f"Predicted {disease} due to symptoms: {patient['symptoms']} and mutation: {patient['dna_mutations']}"

# Main execution
if __name__ == "__main__":
    # Load and preprocess
    real_data, graph_data, model, le_disease = load_data_and_model()
    model.eval()
    with torch.no_grad():
        pred = model(graph_data).argmax(dim=1)

    for i, patient in real_data.iterrows():
        explanation = explain_prediction(patient, pred[i], le_disease)
        print(explanation)

import google.generativeai as genai
import os
import pandas as pd
import json

# Configure Gemini API
def configure_gemini(api_key):
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.0-flash')
    return model

# Load simulated dataset (for context in prompts)
def load_data():
    return pd.read_csv('simulated_rare_disease_data.csv')

# Prompt 1: Why the GNN model selected a specific prediction
def get_why_prediction_prompt(data):
    sample_patient = data.iloc[0]  # Example patient
    return f"""
You are an expert in machine learning and explainable AI, with knowledge of Graph Neural Networks (GNNs) for rare disease detection. A GNN model uses a simulated dataset with patient data, including symptoms (e.g., "{sample_patient['symptoms']}"), DNA mutations (e.g., "{sample_patient['dna_mutations']}"), and disease labels (e.g., "{sample_patient['disease']}"). The GNN represents patients as nodes, connected by edges based on shared symptoms, using BERT embeddings for symptoms and LabelEncoder for DNA mutations. SHAP (Shapley Additive Explanations) provides feature importance.

For a patient with symptoms "{sample_patient['symptoms']}" and DNA mutation "{sample_patient['dna_mutations']}", the GNN predicts "{sample_patient['disease']}". Explain why the model selected this prediction. Describe how the GNN’s graph structure (patient connections via symptoms) and SHAP values highlight the contribution of symptoms and mutations to the prediction. Provide a clear, human-understandable explanation of the model’s decision-making process, focusing on symptom-mutation interactions in the graph.
"""

# Prompt 2: How the GNN model arrived at the prediction
def get_how_prediction_prompt(data):
    sample_patient = data.iloc[0]
    return f"""
You are an expert in Graph Neural Networks (GNNs) for healthcare. A GNN model for rare disease detection uses a simulated dataset with patient data (symptoms like "{sample_patient['symptoms']}", DNA mutations like "{sample_patient['dna_mutations']}", disease labels like "{sample_patient['disease']}"). Patients are nodes in a graph, with edges for shared symptoms. Features include BERT embeddings for symptoms and LabelEncoded DNA mutations. The model uses two GCNConv layers and SHAP for explainability.

For a patient with symptoms "{sample_patient['symptoms']}" and mutation "{sample_patient['dna_mutations']}", the model predicts "{sample_patient['disease']}". Explain how the GNN arrived at this conclusion, including:
1. Graph construction (edge creation based on symptom similarity).
2. Feature aggregation from neighboring nodes using GCNConv layers.
3. How aggregated features lead to the classification.
4. How SHAP values explain feature contributions.

Provide a technical yet clear explanation for someone familiar with machine learning but not GNNs.
"""

# Prompt 3: Next steps for prevention and control
def get_prevention_control_prompt(data):
    sample_patient = data.iloc[0]
    return f"""
You are a medical AI expert specializing in rare disease management. A GNN model for rare disease detection, trained on a simulated dataset with symptoms (e.g., "{sample_patient['symptoms']}"), DNA mutations (e.g., "{sample_patient['dna_mutations']}"), and disease labels (e.g., "{sample_patient['disease']}"), predicts diseases and uses SHAP for explanations. For patients predicted to have "{sample_patient['disease']}", suggest actionable steps to prevent and control the disease, including:
1. Preventive measures (e.g., genetic screening, lifestyle changes).
2. Control measures (e.g., medical interventions, therapies).
3. How the GNN’s predictions and SHAP explanations guide clinicians.
4. Public health strategies (e.g., patient registries, screening programs).

Base suggestions on medical best practices and recent research on rare disease management.
"""

# Main execution
if __name__ == "__main__":
    # Set your Gemini API key (replace with your key or use environment variable)
    API_KEY = os.getenv(' Gemini API key') or ' Gemini API key'

    # Configure Gemini model
    model = configure_gemini(API_KEY)

    # Load simulated dataset for context
    data = load_data()

    # Get prompts
    prompt_why = get_why_prediction_prompt(data)
    prompt_how = get_how_prediction_prompt(data)
    prompt_prevention = get_prevention_control_prompt(data)

    # Send prompts to Gemini API and collect responses
    responses = {}
    for prompt_name, prompt_text in [
        ('why_prediction', prompt_why),
        ('how_prediction', prompt_how),
        ('prevention_control', prompt_prevention)
    ]:
        try:
            response = model.generate_content(prompt_text)
            responses[prompt_name] = response.text
            print(f"\nResponse for {prompt_name}:\n{response.text}")
        except Exception as e:
            print(f"Error processing {prompt_name}: {e}")
            responses[prompt_name] = f"Error: {e}"

    # Save responses to a JSON file
    with open('gemini_responses.json', 'w') as f:
        json.dump(responses, f, indent=4)
    print("\nResponses saved to 'gemini_responses.json'")

import pandas as pd
import numpy as np
import torch
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, TFBertModel
import tensorflow as tf
import json
import os
from google.colab import files
import google.generativeai as genai

# Configure Gemini API
def configure_gemini(api_key):
    try:
        genai.configure(api_key=api_key)
        return genai.GenerativeModel('gemini-1.5-flash')
    except Exception as e:
        print(f"Error configuring Gemini API: {e}")
        return None

# Get Gemini explanation
def get_gemini_explanation(model, symptoms, dna_mutations, predicted_disease):
    if model is None:
        return "Gemini API not configured."
    prompt = (
        f"'{symptoms},{dna_mutations} Based on this symptoms and dna_mutations can you what would be the most probable rare diesease' "
        f"and the predicted rare diesease Provide a one-sentence explanation for why this prediction was made."
    )
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"Error querying Gemini API: {e}")
        return "Failed to get Gemini explanation."

# Load or create simulated dataset
def load_data(file_path='simulated_rare_disease_data.csv'):
    if os.path.exists(file_path):
        return pd.read_csv(file_path)
    else:
        print(f"{file_path} not found. Creating simulated dataset...")
        np.random.seed(42)
        data = {
            'patient_id': range(100),
            'symptoms': ['seizures,weak_legs' if i % 2 == 0 else 'rashes,fatigue' for i in range(100)],
            'dna_mutations': ['CLN3' if i % 2 == 0 else 'SNRPN' for i in range(100)],
            'disease': ['Batten Disease' if i % 2 == 0 else 'Prader-Willi Syndrome' for i in range(100)]
        }
        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False)
        return df

# Preprocess data
def preprocess_data(real_data, new_patient, le_dna=None, le_disease=None):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    bert_model = TFBertModel.from_pretrained('bert-base-uncased')

    def get_symptom_embeddings(symptoms):
        inputs = tokenizer(symptoms, return_tensors='tf', padding=True, truncation=True, max_length=50)
        outputs = bert_model(inputs)
        return outputs.pooler_output.numpy()

    real_embeddings = np.vstack([get_symptom_embeddings(s) for s in real_data['symptoms']])
    if le_dna is None:
        le_dna = LabelEncoder()
        le_dna.fit(real_data['dna_mutations'])
    real_dna = le_dna.transform(real_data['dna_mutations']).reshape(-1, 1)
    if le_disease is None:
        le_disease = LabelEncoder()
        le_disease.fit(real_data['disease'])
    real_labels = le_disease.transform(real_data['disease'])
    X_real = np.hstack([real_embeddings, real_dna])

    new_embedding = get_symptom_embeddings(new_patient['symptoms'])
    try:
        new_dna = le_dna.transform([new_patient['dna_mutations']]).reshape(-1, 1)
    except ValueError:
        print(f"Warning: DNA mutation '{new_patient['dna_mutations']}' not in training data. Using default encoding.")
        new_dna = np.zeros((1, 1))
    X_new = np.hstack([new_embedding, new_dna])

    X = np.vstack([X_real, X_new])
    y = np.concatenate([real_labels, [-1]])
    return X, y, le_dna, le_disease

# Create graph
def create_graph(X, symptoms, new_symptoms):
    edge_index = []
    all_symptoms = symptoms + [new_symptoms]
    for i in range(len(all_symptoms)):
        for j in range(i + 1, len(all_symptoms)):
            if all_symptoms[i] == all_symptoms[j]:
                edge_index.append([i, j])
                edge_index.append([j, i])
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    return Data(x=torch.tensor(X, dtype=torch.float), edge_index=edge_index)

# Define GNN model
class GNN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, num_classes):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return torch.log_softmax(x, dim=1)

# Predict disease
def predict_disease(new_patient, model, graph_data, le_disease):
    model.eval()
    with torch.no_grad():
        pred = model(graph_data).argmax(dim=1)
        new_patient_pred = pred[-1].item()
        try:
            predicted_disease = le_disease.inverse_transform([new_patient_pred])[0]
        except ValueError:
            predicted_disease = "Unknown Disease"
    return predicted_disease

# Generate explanation
def explain_prediction(new_patient, predicted_disease):
    return (
        f"Predicted {predicted_disease} due to symptoms: {new_patient['symptoms']} "
        f"and mutation: {new_patient['dna_mutations']}"
    )

# Main execution
if __name__ == "__main__":


    # Get Gemini API key
    api_key = 'Gemini API key'
    gemini_model = configure_gemini(api_key)

    # Load dataset
    real_data = load_data()
    if real_data is None:
        print("Failed to load dataset. Exiting.")
        exit(1)

    # Initialize GNN model


    # Interactive input loop
    patient_records = []
    while True:
        print("\nEnter patient details (or type 'exit' to finish):")
        symptoms = input("Symptoms (e.g., rashes,fatigue): ").strip()
        if symptoms.lower() == 'exit':
            break
        dna_mutations = input("DNA Mutations (e.g., SNRPN): ").strip()
        if not symptoms or not dna_mutations:
            print("Error: Symptoms and DNA mutations are required.")
            continue

        # Create new patient
        new_patient = {
            'symptoms': symptoms,
            'dna_mutations': dna_mutations
        }

        # Preprocess
        X, y, le_dna, le_disease = preprocess_data(real_data, new_patient)

        # Create graph
        graph_data = create_graph(X, real_data['symptoms'].tolist(), new_patient['symptoms'])

        # Predict
        predicted_disease = predict_disease(new_patient, model, graph_data, le_disease)
        explanation = explain_prediction(new_patient, predicted_disease)
        gemini_explanation = get_gemini_explanation(gemini_model, symptoms, dna_mutations, predicted_disease)

        # Output

        print(f"Prediction {gemini_explanation}")

        # Record
        patient_records.append({
            'symptoms': symptoms,
            'dna_mutations': dna_mutations,
            'predicted_disease': predicted_disease,
            'explanation': explanation,
            'gemini_explanation': gemini_explanation
        })

    # Save records to JSON
    if patient_records:
        output_json = 'patient_predictions.json'
        with open(output_json, 'w') as f:
            json.dump(patient_records, f, indent=4)
        print(f"\nPredictions saved to {output_json}")
        files.download(output_json)
    else:
        print("\nNo predictions made.")

!pip install reportlab

import json
import os
from reportlab.lib.pagesizes import letter
from reportlab.lib import colors
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch

# Read Gemini responses from JSON file
def load_gemini_responses(json_file='gemini_responses.json'):
    try:
        with open(json_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: {json_file} not found. Please run gemini_api_prompts.py first.")
        return None
    except json.JSONDecodeError:
        print(f"Error: Invalid JSON format in {json_file}.")
        return None

# Generate PDF report
def generate_pdf_report(responses, output_file='rare_disease_report.pdf'):
    # Create PDF document
    doc = SimpleDocTemplate(output_file, pagesize=letter, rightMargin=inch, leftMargin=inch, topMargin=inch, bottomMargin=inch)
    styles = getSampleStyleSheet()

    # Define custom styles
    title_style = ParagraphStyle(
        name='TitleStyle',
        fontSize=16,
        leading=20,
        alignment=1,  # Center
        spaceAfter=20,
        fontName='Helvetica-Bold',
        textColor=colors.darkblue
    )
    heading_style = ParagraphStyle(
        name='HeadingStyle',
        fontSize=12,
        leading=16,
        spaceAfter=12,
        fontName='Helvetica-Bold',
        textColor=colors.black
    )
    body_style = ParagraphStyle(
        name='BodyStyle',
        fontSize=10,
        leading=14,
        spaceAfter=8,
        fontName='Helvetica',
        textColor=colors.black
    )

    # Content for the report
    content = []

    # Title
    content.append(Paragraph("Rare Disease Detection Report", title_style))
    content.append(Spacer(1, 0.2 * inch))

    # Date and Author
    content.append(Paragraph("Generated by: AI-Driven Diagnostic System", body_style))
    content.append(Paragraph("Date: May 30, 2025", body_style))
    content.append(Spacer(1, 0.3 * inch))

    # Patient Case Overview
    content.append(Paragraph("Patient Case Overview", heading_style))
    content.append(Paragraph(
        "This report details the analysis of a patient with symptoms 'seizures, weak legs' and DNA mutation 'CLN3', "
        "predicted to have Batten Disease by a Graph Neural Network (GNN) model. The model uses a dataset of patient "
        "symptoms and genetic mutations, augmented with synthetic data. Below, we explain the model's prediction, its "
        "decision-making process, and recommended steps for prevention and control, based on insights from Google Gemini AI.",
        body_style
    ))
    content.append(Spacer(1, 0.2 * inch))

    # Why the Model Predicted Batten Disease
    content.append(Paragraph("Why the Model Predicted Batten Disease", heading_style))
    why_text = responses.get('why_prediction', 'No response available.')
    content.append(Paragraph(why_text.replace('\n', '<br/>'), body_style))
    content.append(Spacer(1, 0.2 * inch))

    # How the Model Arrived at the Prediction
    content.append(Paragraph("How the Model Arrived at the Prediction", heading_style))
    how_text = responses.get('how_prediction', 'No response available.')
    content.append(Paragraph(how_text.replace('\n', '<br/>'), body_style))
    content.append(Spacer(1, 0.2 * inch))

    # Next Steps for Prevention and Control
    content.append(Paragraph("Next Steps for Prevention and Control", heading_style))
    prevention_text = responses.get('prevention_control', 'No response available.')
    content.append(Paragraph(prevention_text.replace('\n', '<br/>'), body_style))
    content.append(Spacer(1, 0.2 * inch))

    # Conclusion
    content.append(Paragraph("Conclusion", heading_style))
    content.append(Paragraph(
        "The GNN model provides a robust prediction of Batten Disease for the patient based on symptoms and genetic data. "
        "The explanations highlight key features (e.g., seizures, CLN3 mutation) driving the prediction, while prevention "
        "and control measures offer actionable guidance for clinicians. For further details, contact the AI diagnostic team.",
        body_style
    ))

    # Build the PDF
    doc.build(content)
    print(f"PDF generated: {output_file}")

# Main execution
if __name__ == "__main__":
    # Load Gemini responses
    responses = load_gemini_responses()
    if responses is None:
        exit(1)

    # Generate PDF report
    generate_pdf_report(responses)



